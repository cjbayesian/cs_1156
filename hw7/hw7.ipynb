{
 "metadata": {
  "name": "",
  "signature": "sha256:89366abc885092e6cca8da01e4efaadd812681bdcc597a9c751fd71d7e2a6e32"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Homework 7\n",
      "Corey Chivers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.backends.backend_pdf import PdfPages\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Question 1\n",
      "#### Validation\n",
      "Split in.dta into training (first 25 examples) and validation (last 10 examples). Train on the 25 examples only, using the validation set of 10 examples to select between five models that apply linear regression to $\\phi_0$ through $\\phi_k$, with $k = 3, 4, 5, 6, 7$. For which model is the classification error on the validation set smallest?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy  as np    \n",
      "\n",
      "indat = np.genfromtxt('http://work.caltech.edu/data/in.dta')\n",
      "outdat =  np.genfromtxt('http://work.caltech.edu/data/out.dta')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at the data first:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_color = np.array(['red' if i==1 else 'blue' for i in indat[:,2]])\n",
      "plt.scatter(indat[:,0],indat[:,1],c=y_color,s=100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5v8Atqhyn+YtqYwAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x106161250>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Holy non-linearly seperable Batman! No fear, we've got a non-linear transformation to apply."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to apply Linear Regression with a non-linear\n",
      "transformation for classification. The nonlinear transformation is given by:\n",
      "    $$\\Phi (x_1, x_2) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, |x_1 - x_2|, |x_1 + x_2|)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use the regularized formula:\n",
      "$$w_{reg} = (Z^T Z + \\lambda I)^{\u22121} Z^T y$$ \n",
      "and compare with the normal linear regression one-step learning formula:\n",
      "$$w_{lin} = (Z^T Z)^{-1} Z^T y$$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def q2_transform(x):\n",
      "    x = np.column_stack((np.ones(x.shape[0]).T,x,x[:,0]**2,x[:,1]**2,x[:,0]*x[:,1],np.abs(x[:,0]-x[:,1]),np.abs(x[:,0]+x[:,1])))\n",
      "    return x\n",
      "\n",
      "def error_rate(w,x,y,predfn):\n",
      "    return (predfn(w,x) != y).mean()\n",
      "\n",
      "def linear_fit(x,y):\n",
      "    '''Let's get our linear algebra on.\n",
      "    Numpy can help!'''\n",
      "    inv = np.linalg.inv # for inverting matrices\n",
      "    return np.dot(np.dot(inv(np.dot(x.T,x)),x.T),y)\n",
      "\n",
      "def linear_reg_fit(x,y,lambda_reg):\n",
      "    '''Let's get our linear algebra on.\n",
      "    Numpy can help!'''\n",
      "    inv = np.linalg.inv # for inverting matrices\n",
      "    dim=x.shape[1]\n",
      "    return np.dot(np.dot(inv(np.dot(x.T,x) + lambda_reg * np.identity(dim) ),x.T),y)\n",
      "\n",
      "def linear_pred(w,x):\n",
      "    return ((np.dot(x,w)  > 0 * 1) - 0.5)*2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First with no regularization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = q2_transform(indat[:,:2])\n",
      "y = indat[:,2]\n",
      "\n",
      "fit = linear_fit(x=x,y=y)\n",
      "print 'E_in:', error_rate(fit,x,y,predfn=linear_pred)\n",
      "\n",
      "\n",
      "x = q2_transform(outdat[:,:2])\n",
      "y = outdat[:,2]\n",
      "\n",
      "print 'E_out:', error_rate(fit,x,y,linear_pred)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "E_in: 0.0285714285714\n",
        "E_out: 0.084\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now with varying $\\lambda$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# k = -3\n",
      "x = q2_transform(indat[:,:2])\n",
      "y = indat[:,2]\n",
      "\n",
      "fit = linear_reg_fit(x=x,y=y,lambda_reg=10e-3)\n",
      "print 'E_in:', error_rate(fit,x,y,predfn=linear_pred)\n",
      "\n",
      "\n",
      "x = q2_transform(outdat[:,:2])\n",
      "y = outdat[:,2]\n",
      "\n",
      "print 'E_out:', error_rate(fit,x,y,linear_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "E_in: 0.0285714285714\n",
        "E_out: 0.084\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# k = 3\n",
      "x = q2_transform(indat[:,:2])\n",
      "y = indat[:,2]\n",
      "\n",
      "fit = linear_reg_fit(x=x,y=y,lambda_reg=10e3)\n",
      "print 'E_in:', error_rate(fit,x,y,predfn=linear_pred)\n",
      "\n",
      "\n",
      "x = q2_transform(outdat[:,:2])\n",
      "y = outdat[:,2]\n",
      "\n",
      "print 'E_out:', error_rate(fit,x,y,linear_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "E_in: 0.428571428571\n",
        "E_out: 0.452\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ks = [2,1,0,-1,-2]\n",
      "for k in ks:\n",
      "    x = q2_transform(indat[:,:2])\n",
      "    y = indat[:,2]\n",
      "    fit = linear_reg_fit(x=x,y=y,lambda_reg=10.0**k)\n",
      "    \n",
      "    x = q2_transform(outdat[:,:2])\n",
      "    y = outdat[:,2]\n",
      "    print k,'E_out:', error_rate(fit,x,y,linear_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 E_out: 0.228\n",
        "1 E_out: 0.124\n",
        "0 E_out: 0.092\n",
        "-1 E_out: 0.056\n",
        "-2 E_out: 0.084\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PLA vs. SVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Question 8"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lineFromW(x,w):\n",
      "    '''Return y for a given x and a set of weights w. For drawing the perceptron'''\n",
      "    # w0 + w1 x + w2 y = 0\n",
      "    # y = (-w0 - w1 x) / w2\n",
      "    y = (-w[0] - w[1] * x) / w[2]\n",
      "    return y\n",
      "\n",
      "def lineFromPoints(p1,p2):\n",
      "    '''Return slope and intercept of a line defined by two points '''\n",
      "    # y = m x + b\n",
      "    m = (p1[1] - p2[1]) / (p1[0] - p2[0])\n",
      "    b = p1[1] - m * p1[0]\n",
      "    return (m,b)\n",
      "\n",
      "def lineFromMB(x,m,b):\n",
      "    '''Return y for a given x, m=slope, and b=y intercept'''\n",
      "    y = m * x + b\n",
      "    return y\n",
      "    \n",
      "def classifyByLine(x,m,b):\n",
      "    '''Return true is below the line, false if above'''\n",
      "    y = x[:,1] > x[:,0] * m + b\n",
      "    return y\n",
      "\n",
      "def classifyByW(x,w):\n",
      "    '''Linear perceptron, implementing h(x)=sign(W^T . x)'''\n",
      "    #s = w[0] + w[1] * x[:,0] + w[2] * x[:,1] \n",
      "    # Let's use matrix notation instead to allow\n",
      "    # generalization to n-dimensional space\n",
      "    x = np.column_stack((np.ones(x.shape[0]).T,x))\n",
      "    s = (w * x).sum(1)\n",
      "    y = (s > 0)\n",
      "    return y\n",
      "\n",
      "def simData(N=20):\n",
      "    ''' Simulate N points separated by a random line '''\n",
      "    x = np.random.rand(N,2)*2-1\n",
      "    \n",
      "    # pick 2 points to draw a line through\n",
      "    sp = np.random.rand(2,2)*2-1\n",
      "    m,b = lineFromPoints(sp[0,:],sp[1,:])\n",
      "    \n",
      "    # define y according to that line\n",
      "    y = classifyByLine(x=x,m=m,b=b)\n",
      "        \n",
      "    return {'x':x,'y':y}\n",
      "\n",
      "def updateW(w,x,y,verbose=True):\n",
      "    '''One step in the PLA'''\n",
      "    # predict y using the current weights (ie classify)\n",
      "    y_perc = np.logical_not(classifyByW(x,w))\n",
      "    y_perc = classifyByW(x,w)\n",
      "    \n",
      "    # Get the missclassified points\n",
      "    incorrect = np.logical_xor(y_perc,y)\n",
      "\n",
      "    if verbose:\n",
      "        print 'N wrong: ', np.sum(incorrect)\n",
      "    \n",
      "    # If there are no missclassified points, then we're done. Yay!\n",
      "    if np.sum(incorrect) == 0:\n",
      "        return w\n",
      "    \n",
      "    # Get the indicies of misclasified points\n",
      "    inc_idx = [i for i,v in enumerate(list(incorrect))  if v]\n",
      "\n",
      "    # Choose one misclasified point at random\n",
      "    inc_idx = random.choice(inc_idx)\n",
      "    \n",
      "    # encode y_perc as {-1,1} rather than {true,false}\n",
      "    y_perc = np.array([1 if yy else -1 for yy in y_perc])\n",
      "    \n",
      "    # Add on the x0 = 1 artificial coordinate\n",
      "    x = np.column_stack((np.ones(x.shape[0]).T,x))\n",
      "\n",
      "    # PLA definition from the slides!\n",
      "    w = w - (y_perc[inc_idx] * x[inc_idx,:])\n",
      "    return w\n",
      "\n",
      "def fit_PLA(x,y,init_w):\n",
      "    new_w = init_w\n",
      "    old_w = new_w + 1\n",
      "    while not numpy.array_equal(old_w, new_w):\n",
      "        old_w = new_w\n",
      "        new_w = updateW(old_w,x,y,verbose=False)\n",
      "        print old_w, new_w\n",
      "    return new_w\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def experiment(N_in=100,N_out=1000,sim_fnc=simData):\n",
      "    # Simulate some random, linearly separable data\n",
      "    N = N_in + N_out\n",
      "    data = sim_fnc(N=N)\n",
      "\n",
      "    x = data['x'][:N_in]\n",
      "    # encode y as {-1,1} rather than {true,false}\n",
      "    y = data['y'][:N_in]\n",
      "    y = np.array([1 if yy else -1 for yy in y])\n",
      "\n",
      "    # Fit a linear perceptron to the data\n",
      "    w = fit_PLA(x,y,init_w=np.zeros(3))\n",
      "    \n",
      "    # Predict y_out from the perceptron\n",
      "    x = data['x'][N_in+1:]\n",
      "    y_PLA = classifyByW(x,w)\n",
      "    print y_PLA\n",
      "    # Evaluate E_out\n",
      "    E_out = np.logical_xor(y_PLA,data['y'][N_in+1:]).mean()\n",
      "    \n",
      "    return E_out\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "experiment()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.] [ 1.          0.0423699   0.66816086]\n",
        "[ 1.          0.0423699   0.66816086] [ 1.          0.0423699   0.66816086]\n",
        "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "0.57257257257257255"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "\n",
      "svm_model = svm.LinearSVC(C=100000)\n",
      "svm_model.fit(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "LinearSVC(C=100000, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm_model.predict(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "array([-1,  1,  1, -1,  1, -1, -1,  1,  1, -1])"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}